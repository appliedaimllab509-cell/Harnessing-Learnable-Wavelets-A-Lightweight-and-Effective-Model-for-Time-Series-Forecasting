# Harnessing-Learnable-Wavelets-A-Lightweight-and-Effective-Model-for-Time-Series-Forecasting

## Abstract
Time series forecasting requires models that can efficiently capture complex temporal dependencies, especially in large-scale and high-dimensional settings.~While Transformer-based architectures excel at modeling long-range dependencies, their quadratic computational complexity poses limitations on scalability and adaptability. To address these challenges, we propose a lightweight model named Haar-TransF, a novel Transformer-inspired architecture that replaces the self-attention module with a learnable Multi-Scale Haar Transform (MSHT) block. The MSHT block, equipped with trainable low- and high-pass haar like filter coefficients, efficiently captures multi-scale approximation and detail components, effectively encoding periodic patterns while suppressing noise, thereby enhancing the modeling of correlations across multiple time series in forecasting tasks. Extensive experiments on several standard forecasting benchmarks demonstrate that Haar-TransF achieves superior predictive accuracy to conventional Transformer-based models, while substantially reducing memory usage for the time series forecasting task.~The obtained experimental results position Haar-TransF as a scalable and resource-efficient framework for advanced time series forecasting.
